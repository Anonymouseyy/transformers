{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1540b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "15832a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset length 1115394'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "f\"dataset length {len(text)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "be35a99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"vocab size: 65; vocab: \\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "f\"vocab size: {vocab_size}; vocab: {''.join(chars)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9aada688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0912b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(text))\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7455cbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self attention head\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # Kinda the metadata contained by each token (how relevant each token is), bias=False - no added constant :)\n",
    "query = nn.Linear(C, head_size, bias=False) # Kinda the information each token is looking for\n",
    "value = nn.Linear(C, head_size, bias=False) # The actual data contained at each token\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "# Similarity between each key and query is the weights\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "wei = wei * C**-0.5 # Scale the weights down so softmax doesn't turn into one hot\n",
    "# Weights are now data dependent\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # (T, T)\n",
    "# Mask future tokens\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\")) # (B, T, T)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# Value tensor, the actual data contained by each token, what is offered by each token\n",
    "v = value(x) # (B, T, 16)\n",
    "\n",
    "# The Query-Key mechanism decides how much of each value should contribute to the final representation of a token.\n",
    "out = wei @ v # (B, T, T) @ (B, T, 16) --> (B, T, 16)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e4dd5f",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "-\"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0fbf8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 100\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5dabcc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size))) # Stored with the model but not treated as trainable weights with register_buffer\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd) # Projection back into residual pathway\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # from (B, T, head_size) to (B, T, C)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # Projection back into residual pathway\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd) # Unit gaussian distribution across layers at init\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x + are residual connections, helps training\n",
    "        x = x + self.sa(self.ln1(x)) # Different from original paper, prenorm\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # Final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "997a968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209.729 K parameters\n"
     ]
    }
   ],
   "source": [
    "model = Transformer()\n",
    "model = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "00d6b267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2702, val loss 4.2704\n",
      "step 500: train loss 0.7172, val loss 0.7535\n",
      "step 1000: train loss 0.1558, val loss 0.1707\n",
      "step 1500: train loss 0.1092, val loss 0.1206\n",
      "step 2000: train loss 0.0936, val loss 0.1011\n",
      "step 2500: train loss 0.0930, val loss 0.0973\n",
      "step 3000: train loss 0.0895, val loss 0.0926\n",
      "step 3500: train loss 0.0894, val loss 0.0919\n",
      "step 4000: train loss 0.0848, val loss 0.0863\n",
      "step 4500: train loss 0.0940, val loss 0.0958\n",
      "step 4999: train loss 0.0826, val loss 0.0864\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d74da44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YOfzKkkkzknnkuu--uuullllllyond is inget undinst she han gould mere ber ansk chrar be, in;er ary theNi.\n",
      "\n",
      "LUARd I bar.\n",
      "Ura chlore, ad thear tue.\n",
      "\n",
      "Yor herit ind gun ta mrin gove! anlan the dimed be be, aunarr qure ager coroud Beamer' herear oorgaard\n",
      "Wautin;\n",
      "Fer tho-wel tord neer Wigel thess ont hrac here! I thel deme hen om hapke!\n",
      "End Enve ye jor tauur t ond to son srity fil tinep coude ayor le, to \n",
      "afr me rand old andomingwticr:\n",
      "Anco om fnebset gong thor;\n",
      "TE wer ndanlangod ind atal ice qxugulid hirde me tond acecingen ond and daar bersed weer ave ed do we, din bey tol; ave:\n",
      "s Ke mof aung tr thor yor mo, muk hinoals ake wer, ice llrecary com,\n",
      "Hares the ore ba.\n",
      "E\n",
      "PUCKILYRD: nor, I Jmern qunto nin he,\n",
      "Fove tut y ree;\n",
      "End my be, sind mine!\n",
      " ingud jon dod, kir! rib'al sr arey\n",
      "Y Longe nro. so mande yonur trend\n",
      "Topem'gs nesg indam yn ton wr bente,\n",
      "Dr Dom mor wom are ther and uard vece ima, \n",
      "And hat ot genill ton baepr oro, nod waved oned va ey ude,ur he hiry.\n",
      "\n",
      "Gor toe ter,\n",
      "I Takr yivest herbicird to an raad nol ler'ved am himed'lse cerier do umew,' ed,\n",
      "BYEon thes antis pore fovenx ne'r ceerd so my, wr Merey to neduhas cingry prongd po fing\n",
      "Lrd Ros wnee whe. ar it tagheen,\n",
      "frrd band weer,\n",
      "Cee; ord rot too u meree,\n",
      "ge, thint por por Doss the llwas, wind the, your reront t het udnoar me, ar\n",
      "Thamr makd unepiloun maclenk.\n",
      "en wiring\n",
      "Ls cinct eorler doB mon, sole mery! ond yond Hind forelsgor tlor tlert Kong jot breel joe,\n",
      "And?\n",
      "\n",
      "OURET:\n",
      "Ther macenste: anVa, dim.\n",
      "\n",
      "T:\n",
      "Ged uord angas? une and mort y at lore, bHoede to oun'nt bel to wis yond he wen yorb:, ainvel thi sind hle me'ded ad:\n",
      "y toran,\n",
      "Uro\n",
      "inns, tor not head winsed aarsqoru fo's pe, mrl wod ut hared move dbeo bet\n",
      "era eny my'e ind mary.\n",
      "\n",
      "AYI I rorut grom,\n",
      "Fen, to Asve wome resg; tor pey bers.\n",
      "\n",
      "Bu more sro' ode ist woy gugrle,\n",
      "Ad an nest\n",
      "ephem my to-omer and nayse ort inod sins, bine cu;\n",
      "\n",
      "Ror ponwsed.\n",
      "\n",
      "A IIit Ideneep ditis beds.\n",
      "Thly A:\n",
      "Fering in ave.\n",
      "Wher.\n",
      "Thar calering mebe, Gitoge  gham' I tong wn hy,\n",
      "Tarr gind yor\n",
      "Annge wrela\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
