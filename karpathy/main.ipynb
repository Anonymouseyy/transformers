{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1540b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15832a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset length 1115394'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "f\"dataset length {len(text)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be35a99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"vocab size: 65; vocab: \\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "f\"vocab size: {vocab_size}; vocab: {''.join(chars)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9aada688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0912b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(text))\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7455cbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self attention head\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # Kinda the metadata contained by each token (how relevant each token is), bias=False - no added constant :)\n",
    "query = nn.Linear(C, head_size, bias=False) # Kinda the information each token is looking for\n",
    "value = nn.Linear(C, head_size, bias=False) # The actual data contained at each token\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "# Similarity between each key and query is the weights\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "wei = wei * C**-0.5 # Scale the weights down so softmax doesn't turn into one hot\n",
    "# Weights are now data dependent\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # (T, T)\n",
    "# Mask future tokens\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\")) # (B, T, T)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# Value tensor, the actual data contained by each token, what is offered by each token\n",
    "v = value(x) # (B, T, 16)\n",
    "\n",
    "# The Query-Key mechanism decides how much of each value should contribute to the final representation of a token.\n",
    "out = wei @ v # (B, T, T) @ (B, T, 16) --> (B, T, 16)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e4dd5f",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "-\"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fbf8e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparams\n",
    "\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 100\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5dabcc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size))) # Stored with the model but not treated as trainable weights with register_buffer\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd) # Projection back into residual pathway\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # from (B, T, head_size) to (B, T, C)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # Projection back into residual pathway\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd) # Unit gaussian distribution across layers at init\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x + are residual connections, helps training\n",
    "        x = x + self.sa(self.ln1(x)) # Different from original paper, prenorm\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # Final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "997a968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209.729 K parameters\n"
     ]
    }
   ],
   "source": [
    "model = Transformer()\n",
    "model = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00d6b267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 0.0880, val loss 0.0954\n",
      "step 500: train loss 0.0838, val loss 0.0873\n",
      "step 1000: train loss 0.0833, val loss 0.0879\n",
      "step 1500: train loss 0.0775, val loss 0.0787\n",
      "step 2000: train loss 0.0832, val loss 0.0924\n",
      "step 2500: train loss 0.0737, val loss 0.0777\n",
      "step 3000: train loss 0.0825, val loss 0.0922\n",
      "step 3500: train loss 0.0755, val loss 0.0771\n",
      "step 4000: train loss 0.0718, val loss 0.0740\n",
      "step 4500: train loss 0.0708, val loss 0.0740\n",
      "step 4999: train loss 0.0788, val loss 0.0859\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d74da44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLLLUUEEESSBMByyBykbbkkkkkk:\n",
      "BY:\n",
      "yould Ry by hend bod doyw cryous lims,\n",
      "whel:\n",
      "Qo wave,\n",
      "Hos noet Menncgrithit yorr.\n",
      "IOU\n",
      "Wirtso dithild em bysez?ra! chulldury'sinty the bhe prepate, ver aiis norerd beins thert ave Jsed: my Meliont hinte siy ss benid bj.\n",
      "Ger therit iw\n",
      "Yf sy Righer ouvivant bl-y-P\n",
      "On:\n",
      "Bitfouldied fleys;-yert th on's cly's, boveny fove pyenerseveny Ged ive!.\n",
      "\n",
      "GUMARDUES:\n",
      "Fend srivery?\n",
      "Whest\n",
      "Gon chGe with tht On yond nonorus ert, mhy cy thir Gin's fallt weivef med vercor this nisintng yuirt En yrincst vipcen you het.\n",
      "GCiry my by fyeo sher fanr:\n",
      "Rid sen thach' fen, that't yousw Manth nay wy tildd ry my benked wirirs ghewl thy goroun ne sirit. IG NENI'd\n",
      "Dfllw:ly, fhouse flinst.\n",
      "\n",
      "HA:\n",
      "shive dast'n'thtimy mort ly la my Oennjys lenidisit, neendvesrat He, mang iy'sd endof bovend Goice sie.\n",
      "JRING:\n",
      "JGDU'G:\n",
      "Df fornte sowh, wirthay, than ginitw monkn! mutn.\n",
      "\n",
      "O Yeis bhenitre hrentvevens\n",
      "D'd sive whyy vestithy thie sth henks:\n",
      "wert bef nendyty whous reensgtht medan whes mpee recmqvincendy tonand; the dingy, hill eve'tong Rimvernist thind, hemrever.I\n",
      "HUS:\n",
      "y ponisy nd yordky:\n",
      "Sel wirven. I G\n",
      "Sof ry wirfers, hen thir:\n",
      "There.\n",
      "Cherexv we byint cmemny lise kerkls!\n",
      "When, meny winsdSd the het you bene meryve.\n",
      "UMSTOnd:i in's\n",
      "Pinct shGuny Send byuly af tour.\n",
      "I s!lly Hint heound fenghe thenstznd nign thy focnsi gals'e ayk.\n",
      "HCHETHUSS:\n",
      "RGESSHNNTERO:\n",
      "\n",
      "I youch chart wouck nten ud fand kny Ind frin bevefse:\n",
      "S wifor:\n",
      "Isins ivtchenb we yire'sl riving.\n",
      "VORWUS:\n",
      "Wer?\n",
      "\n",
      "ORRICERUO:\n",
      "Whivernnd by on naniand youd Nang de sley foom fhmy niewh;\n",
      "Che't theryel be fyeo show thelth- meden.\n",
      "ING\n",
      "MERRYUS:\n",
      "Melivent he thondin, Ka bevesge peseus, th Ande chinthy Merrstenss!ust witis tiv veI:\n",
      "O sly my mud me if hengl fy \n",
      "Grimy muestols oI\n",
      "NGhSy ringhosh thestanis ist fos northesk, lok shy Getig ulounigl'nd lof\n",
      "Dy neves:ld ve bnirveve'd nestire mers, theven fifongho wen! sllint vefiven ondoninsg heveny, totorvence's my-ue ghy, third yous, Ry yoh bild hest her yhount, wend.\n",
      "MCSSI-cllley, srary hod hee yousy ho\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
